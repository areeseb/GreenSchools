# GreenSchools README
This projects was originally completed as a graduate school project to investigate the relationship between public school campus greenery and academic perfomance. To some extent this was a feasibility study on the possibility and ease of using fully open-source tools instead of a proprietary GIS. In the end, I ended up completing both but the proprietary version ended up including a more in depth analysis (see my [website](areeseb.github.io))

## _What does the script do?_
The script performs a simple linear regression between the greenness of public school campuses in the LA Area and the percentage of students at each school who receive free or reduced pricing for lunches (a metric of the economic disadvantage of the students at a school). To do this the script dynamically downloads Sentinel-2 imagery of Los Angeles from an open API (sentinelsat), then mosaics the imagery and calculates NDVI. The script then opens a ESRI geodatabase version of the California School Campus Database and reads the public schools polygon feature class into a GeoPandas Geodataframe and projects/clips it to the study area. It then calculates the average NDVI values of each school campus using zonal statistics (rasterstats), and joins the data back to the Geodataframe. The script then web scrapes 100 values (percentage of students on free or reduced lunch) from the California School Dashboard (caschooldashboard.org) and merges the values into a final Geodataframe containing all the data, it also saves this dataframe to a .geojson for future processing. The final portion is the linear regression which is run on the percentage of students on free or reduced lunch (x value) and the zonal average NDVI values of the school (y value). The output will display a basic plot of the regression analysis, the NDVI image, a printed example of the final Geodataframe, and an r value. 
## _How do I run it?_
The script is designed to run in two seperate modes. The first is the "default" mode, which will run the script in full. This includes querying and downloading the API data, processing the imagery, and scraping the California School Dashboard, and running the regression analysis. PLEASE NOTE that this whole process can take a while and is highly dependent on the users available internet bandwith. With high speed internet >50MBps it will likely run in 5-10 minutes. With slower internet it could take an hour or more. The images alone are around 7GBs. To run the script in default mode, simply type: "$python /path/to/greenschools.py" in a terminal/command window. The script will also run in "static" mode. In this case a static version of the data, stored as a GeoJSON and Gtiff, will be used for processing and display. (NOTE: due to the file size, the github verson cannot be run in static mode). Please reach out if you would like a copy of the static data). In this case, the script will run in a matter of seconds but again, there will be no scraping or downloading of data. To run the script in static mode, simply type: "$python /path/to/greenschools.py --static" in a terminal/command window. IMPORTANT!! Please note that the script has a number of very specific library dependancies and will not run on a base root version of python. To install the dependancies please perform a pip install from the requirements.txt file. To do this type something like: "$ pip install -r requirements.txt" into the terminal/command window. Also note that a few of the libraries are fairly large and may give a few warnings. At most, the install should take ~5 minutes. I have tested the pip install on mac/linux/windows and it has worked each time. It is HIGHLY RECOMMENDED that you set up a virtual python environment for this so as not to mess up any previsouly installed package dependancies. I assume you all are professionals at doing this but if not please read: (www.dabapps.com/blog/introduction-to-pip-and-virtualenv-python/) for assistance. ALSO IMPORTANT!! The web scraping portion of the script has OS dependancies and currently requires the user to input their specific OS (this may change in the future). You will be prompted inside the script to type "mac", "linux", or "windows" depending on your OS. To understand more about what the script is displaying and how long it will take please read the _Data Sources_ portion of the readme.
## _Data Sources_ 
The project has three data sources:

- Source 1: Sentinel-2 imagery
- Source 2: A static ESRI geodatabase
- Source 3: Web scraped data

Source 1 is _Sentinel-2_ imagery downloaded from _European Space Agency's_ Copernicus "sentinelsat" open API. My script first queries the API and saves the query results into a GeoPandas Geodataframe. It then uses the geodataframe to filter the results and download the specific images into the Imagery folder (inside the Data folder). Each image is around 1.5GB in size so the full program only downloads 4 seperate images. Running the script in default mode will query and download all 4 images. NOTE that this could take a while depending on your internet bandwidth. Running the script in static mode will simply display a geodataframe from a full query read in from a GeoJSON file. It will also display the final NDVI geotiff. Also note that there are occationally issues with the API in that case the data will be offline and the default mode of the script will be unavailable. The the data provided can be extremely important for certain industries and is highly regulated.

Source 2 is a static copy of the _California School Campus Database (CDCD)_ developed and distributed by the _Stanford Prevention Research Center_ and _GreenInfo Network_. The database contains accurate geotagged school campus polygons as well as other basic school information such as CDS codes, address, enrollment, and district info. It is in an ESRI Geodatabase format, but is read into a GeoPandas Dataframe for filtering, processing, and display. Running the script in default mode will display a geodataframe of the full CSCD. The --static mode displays a full geodataframe of a static GeoJSON version of part of the database (the part that is useful).

Source 3 is data scraped from the _California School Dashboard_ (caschooldashboard.org). It scrapes data on the percentage of students at schools in the CSCD who received free or reduced lunch. This is a classic metric in the world of education research to indicate the socioeconomic demographic of a school. The data for each school is scraped using the unique CSD code (ID) for each school. The default version of the script will scrape 100 randomly selected schools and display them in a dataframe with the CSD code, the percent disadvantaged, and mean NDVI. The --static version will display a Pandas Dataframe representation of a static JSON containing the scraped data. NOTE that the scraping can take up to 3 seconds per data point so the full default version could take ~5 minutes. 
